#!/usr/bin/env python3
"""Debug duplicate AI evaluation scores issue."""

import sys
import asyncio
import logging
from pathlib import Path

# Add the project root to Python path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from backend.app.services.evaluator import ArticleEvaluator
from backend.app.repositories.article_repository import ArticleRepository
from backend.app.models.article import Article
from backend.app.utils.logger import get_logger

# Enable detailed logging to console
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = get_logger(__name__)


async def test_duplicate_scores():
    """Test AI evaluation to see if it produces duplicate scores."""
    try:
        # Create test articles with different content
        test_articles = [
            Article(
                id="test_1",
                title="„Ç≤„Éº„É†Èü≥Ê•Ω„ÅÆÈ≠ÖÂäõ„Å´„Å§„ÅÑ„Å¶Ë™û„Çã",
                url="https://note.com/test1",
                author="„ÉÜ„Çπ„Éà„É¶„Éº„Ç∂„Éº1",
                published_at="2025-06-15T10:00:00+09:00",
                category="„Ç≤„Éº„É†",
                content_preview="„Ç≤„Éº„É†Èü≥Ê•Ω„ÅØÁßÅ„Åü„Å°„ÅÆÂøÉ„Å´Ê∑±„ÅèÈüø„ÅèË¶ÅÁ¥†„ÅÆ‰∏Ä„Å§„Åß„Åô„ÄÇRPG„ÅÆÁæé„Åó„ÅÑ„Ç™„Éº„Ç±„Çπ„Éà„É©„Åã„Çâ„ÄÅ„Ç¢„ÇØ„Ç∑„Éß„É≥„Ç≤„Éº„É†„ÅÆ„ÉÜ„É≥„Éù„ÅÆËâØ„ÅÑBGM„Åæ„Åß„ÄÅÂ§öÊßò„Å™Èü≥Ê•Ω„Åå„Ç≤„Éº„É†‰ΩìÈ®ì„ÇíË±ä„Åã„Å´„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ"
            ),
            Article(
                id="test_2", 
                title="„Ç§„É≥„Éá„Ç£„Éº„Ç≤„Éº„É†ÈñãÁô∫„ÅÆÁèæÁä∂",
                url="https://note.com/test2",
                author="„ÉÜ„Çπ„Éà„É¶„Éº„Ç∂„Éº2",
                published_at="2025-06-15T11:00:00+09:00",
                category="„Ç≤„Éº„É†",
                content_preview="ËøëÂπ¥„ÄÅ„Ç§„É≥„Éá„Ç£„Éº„Ç≤„Éº„É†Â∏ÇÂ†¥„ÅØÊÄ•ÈÄü„Å´Êã°Â§ß„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÂ∞èË¶èÊ®°„Å™„ÉÅ„Éº„É†„Åß„ÇÇÂâµÈÄ†ÊÄßË±ä„Åã„Å™„Ç≤„Éº„É†„ÇíÂà∂‰Ωú„Åó„ÄÅ‰∏ñÁïå‰∏≠„ÅÆ„Éó„É¨„Ç§„É§„Éº„Å´Â±ä„Åë„Çã„Åì„Å®„ÅåÂèØËÉΩ„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ"
            ),
            Article(
                id="test_3",
                title="„É¨„Éà„É≠„Ç≤„Éº„É†„ÅÆÈ≠ÖÂäõÂÜçÁô∫Ë¶ã",
                url="https://note.com/test3", 
                author="„ÉÜ„Çπ„Éà„É¶„Éº„Ç∂„Éº3",
                published_at="2025-06-15T12:00:00+09:00",
                category="„Ç≤„Éº„É†",
                content_preview="„Éï„Ç°„Éü„Ç≥„É≥„ÇÑ„Çπ„Éº„Éë„Éº„Éï„Ç°„Éü„Ç≥„É≥ÊôÇ‰ª£„ÅÆ„Ç≤„Éº„É†„Å´„ÅØ„ÄÅÁèæ‰ª£„ÅÆ„Ç≤„Éº„É†„Å´„ÅØ„Å™„ÅÑÁã¨Áâπ„ÅÆÈ≠ÖÂäõ„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇÈôê„Çâ„Çå„ÅüÊäÄË°ìÁöÑÂà∂Á¥Ñ„ÅÆ‰∏≠„ÅßÁîü„ÅøÂá∫„Åï„Çå„ÅüÂâµÊÑèÂ∑•Â§´„ÅØ‰ªä„ÇÇËâ≤Ë§™„Åõ„Åæ„Åõ„Çì„ÄÇ"
            )
        ]
        
        evaluator = ArticleEvaluator()
        
        logger.info(f"üîç Testing AI evaluation with {len(test_articles)} different articles")
        
        results = []
        for i, article in enumerate(test_articles, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"üìù TESTING ARTICLE {i}: {article.id}")
            logger.info(f"Title: {article.title}")
            logger.info(f"Content: {article.content_preview[:100]}...")
            logger.info(f"{'='*60}")
            
            # Prepare content and generate prompt
            content = evaluator._prepare_content_for_evaluation(article)
            messages = evaluator._generate_evaluation_prompt(article, content)
            
            logger.info(f"üì§ CALLING AI...")
            
            try:
                # Call AI directly
                response = evaluator.client.chat.completions.create(
                    model=evaluator.groq_settings.get("model", "llama3-70b-8192"),
                    messages=messages,
                    temperature=evaluator.groq_settings.get("temperature", 0.3),
                    max_tokens=evaluator.groq_settings.get("max_tokens", 1000),
                )
                
                raw_response = response.choices[0].message.content
                logger.info(f"üì• RAW AI RESPONSE:")
                logger.info(raw_response)
                
                # Parse response
                parsed_result = evaluator._parse_ai_response(raw_response, article.id)
                
                if parsed_result:
                    logger.info(f"\n‚úÖ PARSED RESULT:")
                    logger.info(f"Article ID: {parsed_result.article_id}")
                    logger.info(f"Quality: {parsed_result.quality_score}")
                    logger.info(f"Originality: {parsed_result.originality_score}")
                    logger.info(f"Entertainment: {parsed_result.entertainment_score}")
                    logger.info(f"Total: {parsed_result.total_score}")
                    logger.info(f"Summary: {parsed_result.ai_summary}")
                    
                    results.append({
                        'article_id': parsed_result.article_id,
                        'scores': f"{parsed_result.quality_score}/{parsed_result.originality_score}/{parsed_result.entertainment_score}",
                        'total': parsed_result.total_score,
                        'summary': parsed_result.ai_summary
                    })
                else:
                    logger.error("‚ùå FAILED TO PARSE RESPONSE")
                
            except Exception as e:
                logger.error(f"‚ùå AI CALL FAILED: {e}")
            
            # Delay between requests to avoid rate limiting
            await asyncio.sleep(3)
        
        # Analyze results
        logger.info(f"\n{'='*80}")
        logger.info("üìä RESULTS ANALYSIS")
        logger.info(f"{'='*80}")
        
        score_patterns = {}
        for result in results:
            pattern = result['scores']
            if pattern in score_patterns:
                score_patterns[pattern].append(result)
            else:
                score_patterns[pattern] = [result]
        
        logger.info(f"Score patterns found:")
        for pattern, articles in score_patterns.items():
            logger.info(f"  {pattern}: {len(articles)} articles")
            if len(articles) > 1:
                logger.warning(f"  ‚ö†Ô∏è  DUPLICATE PATTERN DETECTED: {pattern}")
                for article in articles:
                    logger.info(f"    - {article['article_id']}: {article['summary'][:60]}...")
        
        if len(set(r['scores'] for r in results)) == len(results):
            logger.info("‚úÖ All articles received unique scores - no duplicates detected")
        else:
            logger.warning("‚ö†Ô∏è  Some articles received identical scores!")
        
    except Exception as e:
        logger.error(f"Debug failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_duplicate_scores())