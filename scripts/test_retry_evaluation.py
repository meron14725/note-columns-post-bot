#!/usr/bin/env python3
"""Test the automatic retry evaluation system."""

import sys
import asyncio
import logging
from pathlib import Path

# Import modules using installed package structure

from backend.app.services.evaluator import ArticleEvaluator
from backend.app.models.article import Article
from backend.app.utils.logger import get_logger

# Enable detailed logging to console
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = get_logger(__name__)


async def test_retry_system():
    """Test the retry evaluation system with articles designed to trigger duplicates."""
    try:
        # Create test articles that are likely to get similar scores
        similar_articles = [
            Article(
                id="test_retry_1",
                title="„Ç≤„Éº„É†Èü≥Ê•Ω„ÅÆÂü∫Á§é",
                url="https://note.com/test1",
                author="„ÉÜ„Çπ„Éà„É¶„Éº„Ç∂„Éº1",
                published_at="2025-06-15T10:00:00+09:00",
                category="„Ç≤„Éº„É†",
                content_preview="„Ç≤„Éº„É†Èü≥Ê•Ω„Å´„Å§„ÅÑ„Å¶Âü∫Êú¨ÁöÑ„Å™ÂÜÖÂÆπ„ÇíË™¨Êòé„Åó„Åæ„Åô„ÄÇÈü≥Ê•Ω„ÅÆÂΩπÂâ≤„Å®„Åù„ÅÆÂäπÊûú„Å´„Å§„ÅÑ„Å¶„ÄÇ"
            ),
            Article(
                id="test_retry_2", 
                title="„Ç≤„Éº„É†Èü≥Ê•Ω„ÅÆÂäπÊûú",
                url="https://note.com/test2",
                author="„ÉÜ„Çπ„Éà„É¶„Éº„Ç∂„Éº2",
                published_at="2025-06-15T11:00:00+09:00",
                category="„Ç≤„Éº„É†",
                content_preview="„Ç≤„Éº„É†Èü≥Ê•Ω„Åå„Éó„É¨„Ç§„É§„Éº„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´„Å§„ÅÑ„Å¶Ëß£Ë™¨„Åó„Åæ„Åô„ÄÇÈü≥Ê•Ω„ÅÆÂøÉÁêÜÁöÑÂäπÊûú„Çí‰∏≠ÂøÉ„Å´„ÄÇ"
            ),
            Article(
                id="test_retry_3",
                title="„Ç≤„Éº„É†Èü≥Ê•Ω„ÅÆÊ≠¥Âè≤",
                url="https://note.com/test3", 
                author="„ÉÜ„Çπ„Éà„É¶„Éº„Ç∂„Éº3",
                published_at="2025-06-15T12:00:00+09:00",
                category="„Ç≤„Éº„É†",
                content_preview="„Ç≤„Éº„É†Èü≥Ê•Ω„ÅÆÁô∫Â±ï„Å´„Å§„ÅÑ„Å¶Ê≠¥Âè≤ÁöÑ„Å´ÊåØ„ÇäËøî„Çä„Åæ„Åô„ÄÇÂàùÊúü„Åã„ÇâÁèæ‰ª£„Åæ„Åß„ÅÆÂ§âÈÅ∑„ÇíË™¨Êòé„ÄÇ"
            ),
            Article(
                id="test_retry_4",
                title="„Ç≤„Éº„É†Èü≥Ê•Ω„ÅÆÊäÄË°ì",
                url="https://note.com/test4", 
                author="„ÉÜ„Çπ„Éà„É¶„Éº„Ç∂„Éº4",
                published_at="2025-06-15T13:00:00+09:00",
                category="„Ç≤„Éº„É†",
                content_preview="„Ç≤„Éº„É†Èü≥Ê•ΩÂà∂‰Ωú„ÅÆÊäÄË°ìÁöÑÂÅ¥Èù¢„Å´„Å§„ÅÑ„Å¶Ëß£Ë™¨„Åó„Åæ„Åô„ÄÇÂà∂‰ΩúÊâãÊ≥ï„Å®ÊäÄË°ì„ÅÆÈÄ≤Ê≠©„Å´„Å§„ÅÑ„Å¶„ÄÇ"
            )
        ]
        
        evaluator = ArticleEvaluator()
        
        logger.info(f"üß™ Testing retry evaluation system with {len(similar_articles)} similar articles")
        logger.info("Goal: Trigger duplicate detection and test automatic retry mechanism")
        
        results = []
        for i, article in enumerate(similar_articles, 1):
            logger.info(f"\n{'='*80}")
            logger.info(f"üìù EVALUATING ARTICLE {i}: {article.id}")
            logger.info(f"Title: {article.title}")
            logger.info(f"Content: {article.content_preview}")
            logger.info(f"{'='*80}")
            
            try:
                # Use the main evaluation method which includes retry logic
                evaluation = await evaluator._evaluate_single_article(article)
                
                if evaluation:
                    logger.info(f"\n‚úÖ EVALUATION COMPLETED:")
                    logger.info(f"Article ID: {evaluation.article_id}")
                    logger.info(f"Scores: {evaluation.quality_score}/{evaluation.originality_score}/{evaluation.entertainment_score} = {evaluation.total_score}")
                    logger.info(f"Is Retry: {evaluation.is_retry_evaluation}")
                    if evaluation.is_retry_evaluation:
                        logger.info(f"Retry Reason: {evaluation.retry_reason}")
                        logger.info(f"üîÑ RETRY EVALUATION TRIGGERED!")
                    logger.info(f"Summary: {evaluation.ai_summary[:100]}...")
                    
                    results.append({
                        'article_id': evaluation.article_id,
                        'pattern': f"{evaluation.quality_score}/{evaluation.originality_score}/{evaluation.entertainment_score}",
                        'total_score': evaluation.total_score,
                        'is_retry': evaluation.is_retry_evaluation,
                        'retry_reason': evaluation.retry_reason,
                        'summary': evaluation.ai_summary[:50] + "..." if len(evaluation.ai_summary) > 50 else evaluation.ai_summary
                    })
                else:
                    logger.error(f"‚ùå EVALUATION FAILED for {article.id}")
                
            except Exception as e:
                logger.error(f"‚ùå ERROR during evaluation of {article.id}: {e}")
            
            # Delay between evaluations
            logger.info("‚è±Ô∏è  Waiting 3 seconds before next evaluation...")
            await asyncio.sleep(3)
        
        # Analyze results
        logger.info(f"\n{'='*80}")
        logger.info("üìä RETRY SYSTEM TEST RESULTS")
        logger.info(f"{'='*80}")
        
        logger.info(f"Total evaluations: {len(results)}")
        retry_count = sum(1 for r in results if r['is_retry'])
        logger.info(f"Retry evaluations: {retry_count}")
        
        # Check for score patterns
        patterns = {}
        for result in results:
            pattern = result['pattern']
            if pattern in patterns:
                patterns[pattern].append(result)
            else:
                patterns[pattern] = [result]
        
        logger.info(f"\nScore pattern analysis:")
        duplicate_found = False
        for pattern, articles in patterns.items():
            logger.info(f"  Pattern {pattern}: {len(articles)} articles")
            if len(articles) > 1:
                duplicate_found = True
                logger.warning(f"    ‚ö†Ô∏è  DUPLICATE PATTERN: {pattern}")
                for article in articles:
                    retry_status = "üîÑ RETRY" if article['is_retry'] else "üìù ORIGINAL"
                    logger.info(f"      - {article['article_id']}: {retry_status}")
        
        # Final assessment
        logger.info(f"\n{'='*80}")
        logger.info("üéØ TEST ASSESSMENT")
        logger.info(f"{'='*80}")
        
        if retry_count > 0:
            logger.info(f"‚úÖ SUCCESS: Retry system activated {retry_count} times")
            logger.info("‚úÖ Automatic retry evaluation is working correctly")
        else:
            logger.info("‚ÑπÔ∏è  INFO: No retry evaluations triggered (articles were sufficiently different)")
        
        if duplicate_found and retry_count == 0:
            logger.warning("‚ö†Ô∏è  WARNING: Duplicates detected but no retries triggered - check detection logic")
        elif not duplicate_found:
            logger.info("‚úÖ SUCCESS: All articles received unique scores")
        
        logger.info("\nüìã DETAILED RESULTS:")
        for result in results:
            retry_info = f" (RETRY: {result['retry_reason']})" if result['is_retry'] else ""
            logger.info(f"  {result['article_id']}: {result['pattern']} = {result['total_score']}{retry_info}")
        
    except Exception as e:
        logger.error(f"Test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_retry_system())